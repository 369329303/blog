---
layout: post
title: 如何优化nginx的配置
---

## 如何优化nginx的配置

### Linux系统参数设置
1. **积压队列**

   `net.core.somaxconn`:  
   积压队列中的最大连接数。如果这个值的大于512，那么在nginx配置文件中也要修改listen指令中的backlog参数。  

2. **文件描述符**

   `fs.file-max`:  
   系统级文件描述符限制

   `nofile`:  
   用户级文件描述符限制

3. **可用端口范围**

   `net.ipv4_ip_local_port_range`:  
   当连接数目很大时，通常设置为1024-65000  

### nginx参数设置
1. **worker 进程**

   `worker_processes`:  
   推荐设置成auto,由程序自己来推断应该孵化多少个worker

   `worker_connections`:  
   每个worker能够同时处理的最大连接数

2. **Keepalive 连接**

   `keepalive_requests`:  
   一个keepalive连接上，客户端可以发起的请求数

   `keepalive_timeout`:  
   keepalive连接可以处于空闲状态的最大时间

   `keepalive`:  
   每个worker与上游服务器之间拥有的最大空闲keepalive连接数目。没有默认值。  
   需要配合proxy_http_version 1.1; 和 proxy_set_header Connection ""; 一起使用。

3. **访问日志**

   `access_log`:  
   是否启用访问日志，如果启用还需指定buffer和flush参数。

4. **sendfile**

   `sendfile`:  
   开启sendfile可以提高tcp数据传输速度。  
   当开启gzip的时候，nginx会自动禁止sendfile选项。  

5. **缓存**

    不适合证券行情接口，不做优化

6. **压缩**

   开启后，效果不明显。就默认关闭了。并且传输内容的最大也就300kb左右。

---

### 实验拓扑图
![nginx 反向代理拓扑图]({{site.url}}/images/nginx_reverse_proxy.svg)
图1： nginx反向代理拓扑图

### 完整的配置文件
/etc/sysctl.conf 配置文件:
```ini
# 设置积压队列中的最大连接数，如果大于512。也需要修改nginx的listen指令。
net.core.somaxconn = 10000
# 设置系统级文件描述符最大数量
fs.file-max = 100000
# 设置可用的端口号范围
net.ipv4.ip_local_port_range = 1024 65535
```

/etc/security/limits.conf
```ini
# 设置用户级文件描述符最大数量
*       soft    nofile  1000000
*       hard    nofile  1000000
```

nginx.conf 配置文件:
```nginx
worker_processes  auto;

error_log  /var/log/nginx/error.log;

events {
   # 每一个worker进程能够处理的最大连接数(与客户端的连接数+与上游服务器的连接数)
   worker_connections 8096;
}

http {
   include       mime.types;
   default_type  application/octet-stream;

   sendfile       on;

   # 一个keepalive连接可以发起的最大连接数
   keepalive_requests 1000;

   access_log off;

   # 定义了一个上有服务器集群
   upstream upCluster{
      # 使用最少连接数调度算法
      least_conn;
      # 每个worker进程与上游服务器之间拥有的最大空闲keepalive连接数
      keepalive 100;
      server 10.131.0.3:8000 weight=11;
      server 10.131.0.4:8000 weight=9;
   }

   server {
      # backlog 的值与net.core.somaxconn相同
      listen      9188 backlog=10000;
      location / {
         # 反向代理到上游服务器
         proxy_pass http://upCluster;
         # 不论客户端与nginx之间是长连接还是短连接；nginx和上游服务器之间一律用长连接
         proxy_http_version 1.1;
         proxy_set_header Connection "";
      }   
   }
}   
```

### 实验结果数据
**！！！ 以下数据结果均是在CPU被稳定压在95%以上时测的数据。需要重点关注Requests/sec 这个结果。**
1. 仅仅代理Server1
```console
root@benchmark-tools:~# wrk -c 100 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   523.49ms  288.98ms   1.09s    57.37%
    Req/Sec    47.32     19.78   121.00     68.06%
  5672 requests in 30.05s, 1.09MB read
Requests/sec:    188.78
Transfer/sec:     37.24KB
root@benchmark-tools:~# wrk -c 100 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   519.86ms  289.92ms   1.06s    57.27%
    Req/Sec    47.78     20.15   120.00     69.04%
  5714 requests in 30.04s, 1.10MB read
Requests/sec:    190.21
Transfer/sec:     37.52KB
root@benchmark-tools:~# wrk -c 100 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   520.78ms  287.10ms   1.09s    58.54%
    Req/Sec    47.64     19.26   111.00     70.24%
  5695 requests in 30.04s, 1.10MB read
Requests/sec:    189.57
Transfer/sec:     37.40KB
```

2. 仅仅代理Server2
```console
root@benchmark-tools:~# wrk -c 100 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   590.98ms  298.07ms   1.31s    59.25%
    Req/Sec    41.98     17.81   101.00     59.75%
  5021 requests in 30.04s, 0.97MB read
Requests/sec:    167.13
Transfer/sec:     32.97KB
root@benchmark-tools:~# wrk -c 100 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   617.87ms  293.93ms   1.29s    58.99%
    Req/Sec    40.29     16.09    90.00     64.89%
  4804 requests in 30.04s, 0.93MB read
Requests/sec:    159.93
Transfer/sec:     31.55KB
root@benchmark-tools:~# wrk -c 100 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 100 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   609.00ms  296.35ms   1.36s    59.61%
    Req/Sec    40.70     17.33   121.00     62.07%
  4852 requests in 30.03s, 0.93MB read
Requests/sec:    161.57
Transfer/sec:     31.87KB
```

3. 同时代理Server1 和 Server2
```console
root@benchmark-tools:~# wrk -c 220 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 220 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   573.87ms  292.59ms   1.91s    58.38%
    Req/Sec    94.98     26.76   200.00     75.83%
  11376 requests in 30.04s, 2.19MB read
  Socket errors: connect 0, read 0, write 0, timeout 1
Requests/sec:    378.65
Transfer/sec:     74.69KB
root@benchmark-tools:~# wrk -c 220 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 220 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   570.34ms  293.89ms   1.25s    58.56%
    Req/Sec    95.47     26.36   181.00     75.33%
  11433 requests in 30.04s, 2.20MB read
Requests/sec:    380.61
Transfer/sec:     75.08KB
root@benchmark-tools:~# wrk -c 220 -d 30s -t 4 http://10.131.0.6:9188/webTest
Running 30s test @ http://10.131.0.6:9188/webTest
  4 threads and 220 connections
  Thread Stats   Avg      Stdev     Max   +/- Stdev
    Latency   565.88ms  290.57ms   1.22s    58.27%
    Req/Sec    96.35     26.51   181.00     74.33%
  11537 requests in 30.03s, 2.22MB read
Requests/sec:    384.17
Transfer/sec:     75.78KB
```

4. Server1 和 Server2 上运行的应用程序
```go
package main

import (
	"log"
	"math/rand"
	"net/http"
	"runtime"
	"strconv"
	"time"
)

func main() {
	runtime.GOMAXPROCS(1)
	http.HandleFunc("/webTest", webTest)
	log.Println("Press Ctrl+C to cancel...")
	http.ListenAndServe("0.0.0.0:8000", nil)
}

func webTest(w http.ResponseWriter, r *http.Request) {
	// 随机睡眠，降低CPU抢占率
	rand.Seed(time.Now().UnixNano())
	time.Sleep(time.Duration(rand.Intn(1000)) * time.Millisecond)
	j := 0
	// for循环，提升CPU使用率
	for i := 0; i < 1e7; i++ {
		j += i
	}
	w.Write([]byte("Hello, world! " + strconv.Itoa(j) + "\n"))
	return
}
```

---

#### 参考文档
1. [Tuning NGINX for Performance](https://www.nginx.com/blog/tuning-nginx/)